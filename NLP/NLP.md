# 信息检索 IR

## 1、词对词

抛弃词义和语义

## 2、基于同义词词典

只考虑词义

### 问题

- 难以顺应时代变化
- 人力成本高
- 无法表示的单词的微妙差异

## 词义 + 语义：3 和 4

## 3、基于规则

语法规则、文法规则，词的多义性等问题成为瓶颈

## 4、基于统计

### （1）基于计数的方法

#### 一元模型

单词向量化

#### 二元模型：位置/上下文

① 将单词表示为向量：共现矩阵

② 向量间的相似度：余弦相似度，求夹角，解决词的多义性问题

#### 三元模型：频次

① 点互信息 PMI

- the 和 car
- drive 和 car

每个单词除以其在语料库中出现的频次 => 根据大数定理，当出现的频次足够多时，求的其实是概率 => 概率提供了量化不确定性的方法 => PMI 的公式正是来自信息论中的互信息这个概念（解决词的多义性问题）

PMI 的值越高，表明相关性越强

② 降维

奇异值分解 SVD

#### 缺点

一次性处理全部学习数据，每次均需重新生成共现矩阵、进行 SVD 等一系列操作

### （2）基于推理的方法（从零开始建立联系的方式：神经网络）

#### 优点

- 使用部分学习数据逐步学习，即允许参数的增量学习（mini-batch 学习）
- 理解更复杂的单词之间的模式（king - man + woman = queen）

#### 融合两种方法：计数 + 统计

*GloVe 方法*

#### word2vec 模型 1：CBOW（continuous bag-of-words）

根据**上下文**预测**目标词**的神经网络

建模：
$$
P(w_t|w_{t-1}, w_{t+1})
$$
各个单词的分布式表示：***Win***

#### word2vec 模型 2：skip-gram

根据**目标词**预测**上下文**的神经网络

建模：
$$
P(w_{t-1}, w_{t+1}|w_t)
$$

#### 改进

① 词嵌入 Embedding：输入层 *Win*

② 负采样 Nagative Sampling：中间层 *Wout*

- 使用二分类拟合多分类
- Sigmoid with Loss，反向传播 y - t（t = 1，正确解标签）
- 正例和负例
- 基于概率分布的采样（exp(0.75)）

![image-20220927090530152](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20220927090530152.png)